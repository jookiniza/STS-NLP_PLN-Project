{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYUxOOkCSOEj"
      },
      "outputs": [],
      "source": [
        "# Librerias\n",
        "!pip install -q transformers datasets sentence-transformers evaluate scipy scikit-learn accelerate seaborn matplotlib\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datasets import load_dataset\n",
        "\n",
        "print(\"Descargando dataset STS Benchmark...\")\n",
        "dataset = load_dataset(\"mteb/stsbenchmark-sts\")\n",
        "\n",
        "df_train = pd.DataFrame(dataset['train'])\n",
        "df_dev = pd.DataFrame(dataset['validation'])\n",
        "df_test = pd.DataFrame(dataset['test'])\n",
        "\n",
        "print(\"\\n--- ESTAD√çSTICAS DEL DATASET ---\")\n",
        "print(f\"Cantidad de ejemplos en Train: {len(df_train)}\")\n",
        "print(f\"Cantidad de ejemplos en Validation: {len(df_dev)}\")\n",
        "print(f\"Cantidad de ejemplos en Test: {len(df_test)}\")\n",
        "\n",
        "print(\"\\n--- EJEMPLO DE DATOS ---\")\n",
        "print(df_train[['sentence1', 'sentence2', 'score']].head())\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(df_train['score'], bins=20, kde=True, color='blue')\n",
        "plt.title(\"Distribuci√≥n de las puntuaciones de similitud (Conjunto de Train)\")\n",
        "plt.xlabel(\"Puntuaci√≥n de Similitud (0.0 a 5.0)\")\n",
        "plt.ylabel(\"Frecuencia\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKCrOhG4aKon"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu4xhdjvaT26"
      },
      "source": [
        "OBJETIVO Z1\n",
        "Implementaci√≥n del c√≥digo para N-Gramas (contar palabras coincidentes) y Sentence Embeddings (usar IA pre-enternada).\n",
        "\n",
        "**IMPORTANTE**: Ngramas compara letras (mismas palabras - no sinonimos), mientras que embedding compara las ideas (significado de las palabras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VkuxuTEaSCm"
      },
      "outputs": [],
      "source": [
        "# BLOQUE: OBJETIVO Z1 (MODELOS NO SUPERVISADOS)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Funci√≥n auxiliar para evaluar (Calcula Pearson)\n",
        "def evaluate_pearson(predictions, references):\n",
        "    pearson_corr, _ = pearsonr(predictions, references)\n",
        "    return pearson_corr\n",
        "\n",
        "test_sentences1 = df_test['sentence1'].tolist()\n",
        "test_sentences2 = df_test['sentence2'].tolist()\n",
        "gold_scores = df_test['score'].tolist()\n",
        "\n",
        "# --- MODELO A: N-GRAMAS (Baseline b√°sica) ---\n",
        "print(\"1. Ejecutando modelo de N-Gramas...\")\n",
        "\n",
        "vectorizer = CountVectorizer().fit(test_sentences1 + test_sentences2)\n",
        "vecs1 = vectorizer.transform(test_sentences1)\n",
        "vecs2 = vectorizer.transform(test_sentences2)\n",
        "\n",
        "ngram_preds = []\n",
        "for i in range(len(test_sentences1)):\n",
        "    sim = cosine_similarity(vecs1[i], vecs2[i])[0][0]\n",
        "    ngram_preds.append(sim * 5)\n",
        "\n",
        "# --- MODELO B: SENTENCE EMBEDDINGS (Baseline avanzada) ---\n",
        "print(\"2. Ejecutando modelo de Sentence Embeddings (all-MiniLM-L6-v2)\")\n",
        "\n",
        "model_st = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "embeddings1 = model_st.encode(test_sentences1, convert_to_tensor=True)\n",
        "embeddings2 = model_st.encode(test_sentences2, convert_to_tensor=True)\n",
        "\n",
        "\n",
        "from sentence_transformers import util\n",
        "cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
        "\n",
        "st_preds = []\n",
        "for i in range(len(test_sentences1)):\n",
        "    st_preds.append(cosine_scores[i][i].item() * 5)\n",
        "\n",
        "# EVALUACI√ìN FINAL Z1\n",
        "print(\"\\n RESULTADOS Z1 (Correlaci√≥n de Pearson)\")\n",
        "#LES PASAMOS LAS NOTAS PREDICHAS PARA NGRAMS Y PARA ST Y LOS RESULTADOS REALES\n",
        "pearson_ngram = evaluate_pearson(ngram_preds, gold_scores)\n",
        "pearson_st = evaluate_pearson(st_preds, gold_scores)\n",
        "\n",
        "print(f\"Modelo N-Gramas: {pearson_ngram:.4f}\")\n",
        "print(f\"Modelo Sentence Embeddings: {pearson_st:.4f}\")\n",
        "\n",
        "results_table = {\n",
        "    \"Modelo\": [\"N-Grams (Z1)\", \"Sentence Embeddings (Z1)\"],\n",
        "    \"Pearson\": [pearson_ngram, pearson_st]\n",
        "}\n",
        "pd.DataFrame(results_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNJa0gFnaPuN"
      },
      "source": [
        "**Z2: AJUSTAR EL MODELO A LA TAREA UTILIZANDO LOS DATOS DE ENTRENAMIENTO (FINE-TUNNING)**\n",
        "\n",
        "A utilizar: Cross-Encoder, como entrada se le dan dos frases juntas separadas por una etiqueta [SEP]. Despu√©s, como cerebro se usar√° BERT con su mecanismo de auto atenci√≥n para mirar cada palabra de A y compararla con cada palabra de B. Y, finalmente, le pediremos que escupa un solo n√∫mero (regresi√≥n) que represente la similitud.\n",
        "\n",
        "BLOQUE 2.1: PREPARAR LOS DATOS PARA EL EXAMEN\n",
        "Vaciamos la memorioa de la tarjeta grafica y cargamos el dataset de 0 para evitar columnas fantasma que puedan darnos problemas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fya1QqnKaQhx"
      },
      "outputs": [],
      "source": [
        "# --- BLOQUE Z2 - PASO 1: LIMPIEZA Y CARGA ---\n",
        "import torch\n",
        "import gc\n",
        "from datasets import load_dataset\n",
        "\n",
        "#Limpiar la memoria de la GPU a la fuerza\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\" Memoria de GPU limpiada.\")\n",
        "\n",
        "#Cargar el dataset limpio desde cero\n",
        "dataset = load_dataset(\"mteb/stsbenchmark-sts\")\n",
        "print(\"Dataset cargado correctamente.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGkKVmFPjBcK"
      },
      "source": [
        "BLOQUE 2.2: PREPOROCESADO INTELIGENTE\n",
        "\n",
        "El modelo BERT necesita que las frases est√©n convertidas en n√∫meros, y el Trainer exige que la columna de notas se llame Label para poder procesarla. As√≠, en este bloque tokenizamos frases juntas, renonbramos y detectamos columnas que sobren para borrarlas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjqbBhF9jaT0"
      },
      "outputs": [],
      "source": [
        "# BLOQUE Z2 - PASO 2: PREPROCESADO\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# 1. Cargamos el tokenizador\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "# 2. Funci√≥n para preparar los datos\n",
        "def preprocess_function(examples):\n",
        "    # Tokenizamos las dos frases juntas (Frase A [SEP] Frase B)\n",
        "    tokenized = tokenizer(examples['sentence1'], examples['sentence2'], truncation=True, max_length=128)\n",
        "    tokenized['label'] = [float(score) for score in examples['score']]\n",
        "    return tokenized\n",
        "\n",
        "# 3. Aplicamos la transformaci√≥n\n",
        "print(\"Tokenizando datos\")\n",
        "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "columnas_necesarias = ['input_ids', 'token_type_ids', 'attention_mask', 'label']\n",
        "columnas_existentes = encoded_dataset[\"train\"].column_names\n",
        "columnas_a_borrar = [col for col in columnas_existentes if col not in columnas_necesarias]\n",
        "\n",
        "print(f\"Borrado de columnas innecesarias: {columnas_a_borrar}\")\n",
        "encoded_dataset = encoded_dataset.remove_columns(columnas_a_borrar)\n",
        "\n",
        "# 5. Formato final para PyTorch\n",
        "encoded_dataset.set_format(\"torch\")\n",
        "print(\"Datos listos para el entrenamiento.\")\n",
        "print(\"Ejemplo de entrada:\", encoded_dataset['train'][0].keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sSMeMfgjk3v"
      },
      "source": [
        "BLOQUE 3: CONFIGURAMOS EL ENTRENADOR\n",
        "\n",
        "Voy a definir el modelo, metrica y reglas que vamos a usar para el entrenamiento. Lo voy a hacer con una configuraci√≥n de tama√±o de batch de 8 para asegurar que no se sature la memoria de collab, pero usando a su vez acumulazion de gradiantes que le va a yudar en el aprendizaje.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FR6fPk6_j5Hc"
      },
      "outputs": [],
      "source": [
        "# BLOQUE Z2 - PASO 3: MODELO Y CONFIGURACI√ìN\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from scipy.stats import pearsonr\n",
        "import numpy as np\n",
        "\n",
        "# 1. Cargamos el modelo (num_labels=1 para Regresi√≥n)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=1)\n",
        "\n",
        "# 2. Definimos la m√©trica (Pearson)\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    # Protecci√≥n por si devuelve una tupla extra√±a\n",
        "    if isinstance(predictions, tuple):\n",
        "        predictions = predictions[0]\n",
        "    # Aplanamos y calculamos Pearson\n",
        "    return {\"pearson\": pearsonr(predictions.flatten(), labels.flatten())[0]}\n",
        "\n",
        "# 3. Argumentos de Entrenamiento mejores\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"modelo_z2_final\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,              # Variable que va cambiando para el procesamiento de distintas pruebas\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=4,   # CAMBIO TAMBIEN (EN BATCH DE 16 A 32 PORQUE LA MULTIPLICACION SE HAR√Å CON ESTE)\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=10,              # los epoches que le damos -cuantas vueltas- (CAMBIA)\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,     # Quedarnos con el mejor\n",
        "    metric_for_best_model=\"pearson\",\n",
        "    report_to=\"none\",\n",
        "    fp16=True\n",
        ")\n",
        "\n",
        "# 4. Inicializamos el Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "print(\"Entrenador configurado\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGZUr19QmVsC"
      },
      "source": [
        "2.4: ENTRENAMIENTO\n",
        "\n",
        "Vamos a entrenar con los parametros que hemos metido antes y los datos que hemos adecuado y vamos a ver como reacciona nuestro sistema con la nota que nos da\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aa6EC8bJmftP"
      },
      "outputs": [],
      "source": [
        "# --- BLOQUE Z2 - PASO 4: ENTRENAMIENTO Y RESULTADO ---\n",
        "\n",
        "print(\"Iniciando entrenamiento Z2\")\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\nEntrenamiento finalizado. Evaluando en el conjunto de TEST...\")\n",
        "# Hacemos la predicci√≥n final sobre el examen real (Test)\n",
        "test_results = trainer.predict(encoded_dataset[\"test\"])\n",
        "pearson_final = test_results.metrics['test_pearson']\n",
        "\n",
        "print(\"=\"*40)\n",
        "print(f\"RESULTADO FINAL Z2 (BERT Fine-Tuning): {pearson_final:.4f}\")\n",
        "print(\"=\"*40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPmDmb7W38-v"
      },
      "source": [
        "**CAMBIO DE ARQUITECTURA PARA VER CUAL NOS BENEFICIA M√ÅS CON RESPECTO A LO QUE TENEMOS HASTA AHORA:**\n",
        "\n",
        "_CAMBIO 1: OPTIMIZACI√ìN DEL BERT INICIAL_\n",
        "\n",
        "Las correcciones que he aplicado a este bloque son las siguientes: por un lado, le he agregado un early stopping (frente al solo Load_best_model_at_end con el que contaba al principio) y luego he agregado una semilla para que sea reproducible. Adem√°s, le he agregado un learnin rate est√°tico, con un warmup de 0.1, que puede ayudarme a progresar m√°s adecuadamente en los experiemntos.\n",
        "\n",
        "Tambi√©n le he agregado un seed para que cada vez que inicialice con los mismos par√°metros mirando a futuras pruebas o entrenamientos que se puedan hacer sobre el c√≥digo, obteniendo as√≠ resultados consistentes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4MWLQmYFl_J"
      },
      "outputs": [],
      "source": [
        "# --- BLOQUE 2.5: BERT OPTIMIZADO (Mejoras de Hiperpar√°metros) ---\n",
        "from transformers import EarlyStoppingCallback, set_seed\n",
        "\n",
        "# 1. Fijo semilla para reproducibilidad\n",
        "set_seed(42)\n",
        "\n",
        "# 2. Argumentos de Entrenamiento Mejorados\n",
        "args_optimized = TrainingArguments(\n",
        "    output_dir=\"modelo_z2_bert_opt\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,              # Mantengo el LR con el que conseguimos mejores resultados\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=4,   # Batch de 32 (8x4)\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=10,             # Le doy margen de 10 epoches, aunque con el early stopping parar√°.\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.1,                # WARMUP: suaviza aprendizaje para evitar overfitting\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"pearson\",\n",
        "    greater_is_better=True,\n",
        "    save_total_limit=2,              # Ahorra espacio guardando solo los 2 mejores\n",
        "    report_to=\"none\",\n",
        "    fp16=True\n",
        ")\n",
        "\n",
        "# 3. Reinicializar el modelo para entrenarlo de 0\n",
        "model_opt = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=1)\n",
        "\n",
        "# 4. Trainer con Early Stopping\n",
        "trainer_opt = Trainer(\n",
        "    model=model_opt,\n",
        "    args=args_optimized,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset[\"validation\"],\n",
        "    processing_class=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]) # EARLY STOPPING para que pare si no ve mejoras en 3 √©pocas\n",
        "\n",
        "print(\"Inicio entrenamiento BERT Optimizado\")\n",
        "trainer_opt.train()\n",
        "\n",
        "# 5. Evaluaci√≥n Final\n",
        "print(\"\\n Evaluando BERT Optimizado en TEST\")\n",
        "results_opt = trainer_opt.predict(encoded_dataset[\"test\"])\n",
        "print(f\"RESULTADO FINAL (BERT Optimizado): {results_opt.metrics['test_pearson']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLgmKaezH2aT"
      },
      "source": [
        "**EXPERIMENTO CON OTRA ARQUITECTURA: RoBERTa**\n",
        "\n",
        "Prueba con Roberta para optimizar el pre-entrenamiento de BERT (elimina el NSP, usa dynamic masking) y es la base arquitectonica del modelo multiling√ºe de XLM-ROBERTA, que es el que estar√© utilizando para el modelo de Z3.\n",
        "\n",
        "\n",
        "Roberta es la evoluci√≥n directa de BERT. Los autores de RoBERTa demostraron que BERT estaba \"sub-entrenado\". Al quitar la tarea de predicci√≥n de la siguiente frase (NSP) y entrenar con m√°s datos, lograron un rendimiento muy superior en tareas sem√°nticas como STS.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAafmh5lLXZa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gc\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# 1. LIMPIEZA DE MEMORIA\n",
        "del model_opt, trainer_opt  # Borro los objetos anteriores si existen\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"Memoria liberada. Preparando experimento RoBERTa\")\n",
        "\n",
        "# 2. CONFIGURACI√ìN DEL NUEVO MODELO\n",
        "model_checkpoint = \"roberta-base\"\n",
        "print(f\"Cargando {model_checkpoint}\")\n",
        "\n",
        "tokenizer_roberta = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model_roberta = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=1)\n",
        "\n",
        "# 3. PREPROCESADO ESPEC√çFICO\n",
        "def preprocess_roberta(examples):\n",
        "    return tokenizer_roberta(\n",
        "        examples['sentence1'],\n",
        "        examples['sentence2'],\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "print(\"Tokenizando dataset para RoBERTa\")\n",
        "encoded_dataset_roberta = dataset.map(preprocess_roberta, batched=True)\n",
        "\n",
        "# Convertir score a float y formateamos\n",
        "encoded_dataset_roberta = encoded_dataset_roberta.map(\n",
        "    lambda x: {'label': float(x['score'])}\n",
        ")\n",
        "# Eliminar columnas que no sean las necesarias\n",
        "cols_to_keep = ['input_ids', 'attention_mask', 'label']\n",
        "cols_to_remove = [c for c in encoded_dataset_roberta['train'].column_names if c not in cols_to_keep]\n",
        "encoded_dataset_roberta = encoded_dataset_roberta.remove_columns(cols_to_remove)\n",
        "encoded_dataset_roberta.set_format(\"torch\")\n",
        "\n",
        "# 4. CONFIGURACI√ìN DE ENTRENAMIENTO\n",
        "\n",
        "args_roberta = TrainingArguments(\n",
        "    output_dir=\"modelo_z2_roberta_experiment\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,              # Mantener LR exitoso\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=4,   # Batch efectivo 32\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=10,             # Con Early Stopping\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.1,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"pearson\",\n",
        "    greater_is_better=True,\n",
        "    save_total_limit=1,\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer_roberta = Trainer(\n",
        "    model=model_roberta,\n",
        "    args=args_roberta,\n",
        "    train_dataset=encoded_dataset_roberta[\"train\"],\n",
        "    eval_dataset=encoded_dataset_roberta[\"validation\"],\n",
        "    tokenizer=tokenizer_roberta, # Pasar el tokenizer nuevo\n",
        "    compute_metrics=compute_metrics, # Reusar funci√≥n de m√©tricas\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "\n",
        "# 5. EJECUCI√ìN\n",
        "print(\"Entrenando RoBERTa-base\")\n",
        "trainer_roberta.train()\n",
        "\n",
        "# 6. EVALUACI√ìN COMPARATIVA\n",
        "print(\"\\n RESULTADOS EXPERIMENTO ROBERTA (TEST SET):\")\n",
        "results_roberta = trainer_roberta.predict(encoded_dataset_roberta[\"test\"])\n",
        "pearson_roberta = results_roberta.metrics['test_pearson']\n",
        "\n",
        "print(f\"Pearson RoBERTa: {pearson_roberta:.4f}\")\n",
        "print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNisKQrjBb1Q"
      },
      "source": [
        "**Z3: TRANSFERENCIA ENTRE LENGUAS**\n",
        "\n",
        "El objetivo de esta √∫tlima parte es ver si es posible aplicar lo aprendido en un idioma (ingl√©s) a otro que no ha sido visto durante el entrenamiento.\n",
        "\n",
        "\n",
        "_BLOQUE 3.1: CARGA DE DATOS EN ESPA√ëOL Y EVALUACION DEL MODELO Z1 Y Z2 (RoBERTAa y BERT) PARA ESTE IDIOMA_\n",
        "\n",
        "\n",
        "Lo que quiero demostrar con la utilizaci√≥n de estos modelos es que un modelo monoling√ºe no funciona en espa√±ol, significando esto que es necesario la utilizaci√≥n de modelos monoling√ºes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoHd7vGr9aqQ"
      },
      "source": [
        "**Carga de datos en Espa√±ol**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGnY9lV19dW9"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "# 1. Cargar el dataset de Test en Espa√±ol\n",
        "# Usar 'stsb_multi_mt', que es el est√°ndar para STS multiling√ºe\n",
        "print(\"Cargando dataset de test en Espa√±ol (stsb_multi_mt)...\")\n",
        "try:\n",
        "    dataset_es = load_dataset(\"stsb_multi_mt\", name=\"es\", split=\"test\")\n",
        "    print(\"Dataset stsb_multi_mt cargado correctamente.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error cargando stsb_multi_mt: {e}\")\n",
        "    print(\"Intentando cargar desde mteb/stsbenchmark-sts (configuraci√≥n 'es')...\")\n",
        "    dataset_es = load_dataset(\"mteb/stsbenchmark-sts\", \"es\", split=\"test\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOobFWqO9les"
      },
      "source": [
        "1. PRUEBA DE LOS DATOS MULTILING√úES CON LOS ENFOQUES NO SUPERVISADOS:\n",
        "  * N-Gramas: Para ver si las frases se parecen solo por compartir palabras.\n",
        "\n",
        "* Sentence Embeddings: Usar√© el modelo distiluse-base-multilingual-cased-v2 como referencia.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XIorYip-MdT"
      },
      "outputs": [],
      "source": [
        "\n",
        "# BLOQUE 3.1b: EVALUACI√ìN DE ENFOQUES NO SUPERVISADOS (Z1) EN ESPA√ëOL\n",
        "# Objetivo: Evaluar N-Gramas y el modelo Z1 original sobre el test en Espa√±ol.\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from scipy.stats import pearsonr\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "\n",
        "test_sentences1_es = list(dataset_es['sentence1'])\n",
        "test_sentences2_es = list(dataset_es['sentence2'])\n",
        "\n",
        "\n",
        "label_col = 'similarity_score' if 'similarity_score' in dataset_es.features else 'score'\n",
        "gold_scores_es = list(dataset_es[label_col])\n",
        "\n",
        "print(f\"Datos listos: {len(test_sentences1_es)} pares de frases.\")\n",
        "\n",
        "# --- EXPERIMENTO A: N-GRAMAS (Espa√±ol) ---\n",
        "print(\"1. Ejecutando N-Gramas en Espa√±ol\")\n",
        "\n",
        "vectorizer_es = CountVectorizer().fit(test_sentences1_es + test_sentences2_es)\n",
        "vecs1_es = vectorizer_es.transform(test_sentences1_es)\n",
        "vecs2_es = vectorizer_es.transform(test_sentences2_es)\n",
        "\n",
        "ngram_preds_es = []\n",
        "for i in range(len(test_sentences1_es)):\n",
        "    # Similitud coseno\n",
        "    sim = cosine_similarity(vecs1_es[i], vecs2_es[i])[0][0]\n",
        "    ngram_preds_es.append(sim * 5)\n",
        "\n",
        "pearson_ngram_es = pearsonr(ngram_preds_es, gold_scores_es)[0]\n",
        "\n",
        "# --- EXPERIMENTO B: SENTENCE EMBEDDINGS Z1 ---\n",
        "z1_model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "print(f\"2. Ejecutando modelo Z1 ({z1_model_name}) en Espa√±ol\")\n",
        "\n",
        "model_st_z1 = SentenceTransformer(z1_model_name)\n",
        "\n",
        "# Codificamos\n",
        "embeddings1_es = model_st_z1.encode(test_sentences1_es, convert_to_tensor=True)\n",
        "embeddings2_es = model_st_z1.encode(test_sentences2_es, convert_to_tensor=True)\n",
        "\n",
        "# Calculamos similitud\n",
        "cosine_scores_es = util.cos_sim(embeddings1_es, embeddings2_es)\n",
        "\n",
        "st_preds_es = []\n",
        "for i in range(len(test_sentences1_es)):\n",
        "    st_preds_es.append(cosine_scores_es[i][i].item() * 5)\n",
        "\n",
        "pearson_st_es = pearsonr(st_preds_es, gold_scores_es)[0]\n",
        "\n",
        "# --- RESULTADOS ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"RESULTADOS BASALES EN ESPA√ëOL (Subconjunto Z1):\")\n",
        "print(f\"1. N-Gramas (Coincidencia l√©xica): {pearson_ngram_es:.4f}\")\n",
        "print(f\"2. Embeddings Z1 (all-MiniLM - Ingl√©s): {pearson_st_es:.4f}\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gClX2Mk1-M1w"
      },
      "source": [
        "2. EVALUACI√ìN CON ROBERTA (Z2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXmKzHZxJVia"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "# 2. Preprocesar datos en Espa√±ol para RoBERTa\n",
        "# IMPORTANTE: Uso el tokenizador de RoBERTa (que solo sabe ingl√©s) para ver c√≥mo falla al intentar entender espa√±ol.\n",
        "def preprocess_es_roberta(examples):\n",
        "    tokenized = tokenizer_roberta(\n",
        "        examples['sentence1'],\n",
        "        examples['sentence2'],\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "    label_col = 'similarity_score' if 'similarity_score' in examples else 'score'\n",
        "    tokenized['label'] = [float(score) for score in examples[label_col]]\n",
        "    return tokenized\n",
        "\n",
        "print(\"Tokenizando espa√±ol con tokenizador de RoBERTa en ingl√©s\")\n",
        "encoded_es_roberta = dataset_es.map(preprocess_es_roberta, batched=True)\n",
        "encoded_es_roberta.set_format(\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "# 3. Evaluaci√≥n Zero-Shot (Control Negativo)\n",
        "print(\"\\nEvaluando RoBERTa (Ingl√©s) en datos Espa√±ol\")\n",
        "results_control = trainer_roberta.predict(encoded_es_roberta)\n",
        "pearson_control = results_control.metrics['test_pearson']\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(f\"RESULTADO CONTROL (RoBERTa Ingl√©s en Espa√±ol): {pearson_control:.4f}\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOV7kLDG5ASv"
      },
      "source": [
        "Al ver que me da bastante alto, le he metido prints para ver que era lo que esta utlizando al codificar, puesto que a veces se realiza una traduccion y podr√≠a ser por eso que diese los datos tan altos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZgNDZvLcFUN"
      },
      "outputs": [],
      "source": [
        "# VERIFICACI√ìN DE DATOS\n",
        "print(\"--- Muestra de datos de Test ---\")\n",
        "print(f\"Frase 1 (Original): {dataset_es[0]['sentence1']}\")\n",
        "print(f\"Frase 2 (Original): {dataset_es[0]['sentence2']}\")\n",
        "print(f\"Score: {dataset_es[0]['similarity_score'] if 'similarity_score' in dataset_es[0] else dataset_es[0]['score']}\")\n",
        "\n",
        "# Ver qu√© tokens ve el modelo\n",
        "print(\"\\n--- Lo que ve el Tokenizador ---\")\n",
        "input_ids_ejemplo = encoded_es_roberta[0]['input_ids']\n",
        "print(f\"Tokens decodificados: {tokenizer_roberta.decode(input_ids_ejemplo)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1Lo6LOe3CLX"
      },
      "source": [
        "3. EVALUACION DEL MODELO Z2 (BERT) PARA ESTE IDIOMA:\n",
        "\n",
        "Como he limpiado antes de seguir con este bloque nos va a dar error asi que hay que volver a ejecutar el Z2 de BERT para volver a conseguir los datos que necesitamos para proceder con al ejecuci√≥n de BERT para espa√±ol\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJc2UqAT4TX9"
      },
      "outputs": [],
      "source": [
        "#Carga del dataset en espa√±ol\n",
        "if 'dataset_es' not in globals():\n",
        "    from datasets import load_dataset\n",
        "    print(\"Cargando dataset en Espa√±ol...\")\n",
        "    try:\n",
        "        dataset_es = load_dataset(\"stsb_multi_mt\", name=\"es\", split=\"test\")\n",
        "    except:\n",
        "        dataset_es = load_dataset(\"mteb/stsbenchmark-sts\", \"es\", split=\"test\")\n",
        "\n",
        "#Preprocesar Espa√±ol con el tokenizador de BERT\n",
        "def preprocess_es_bert_trained(examples):\n",
        "    tokenized = tokenizer(\n",
        "        examples['sentence1'],\n",
        "        examples['sentence2'],\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "    label_key = 'similarity_score' if 'similarity_score' in examples else 'score'\n",
        "    tokenized['label'] = [float(s) for s in examples[label_key]]\n",
        "    return tokenized\n",
        "\n",
        "print(\"Tokenizando espa√±ol con el tokenizador de BERT\")\n",
        "encoded_es_bert_tuned = dataset_es.map(preprocess_es_bert_trained, batched=True)\n",
        "encoded_es_bert_tuned.set_format(\"torch\", columns=['input_ids', 'attention_mask', 'token_type_ids', 'label'])\n",
        "\n",
        "print(\"\\nüá™üá∏ Evaluando tu BERT Optimizado (Z2) en datos Espa√±o\")\n",
        "results_bert_tuned = trainer_opt.predict(encoded_es_bert_tuned)\n",
        "pearson_bert_tuned = results_bert_tuned.metrics['test_pearson']\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(f\"RESULTADO BERT OPTIMIZADO (Z2) EN ESPA√ëOL: {pearson_bert_tuned:.4f}\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNnPdfwP7Vrt"
      },
      "source": [
        "_3.2 ENTRENAMIENTO DEL MODELO MULTILING√úE: FINE-TUNING CROSS-LINGUAL CON XLM-ROBERTA:_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykUiMi8L8B8t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gc\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    EarlyStoppingCallback,\n",
        "    set_seed\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from scipy.stats import pearsonr\n",
        "import numpy as np\n",
        "\n",
        "# 1. LIMPIEZA DE MEMORIA\n",
        "# Borrar variables de modelos anteriores si existen para liberar la GPU\n",
        "for var in ['model_opt', 'trainer_opt', 'model_roberta', 'trainer_roberta', 'model_bert_audit', 'trainer_xlm']:\n",
        "    if var in globals(): del globals()[var]\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "set_seed(42)\n",
        "print(\"Memoria liberada\")\n",
        "\n",
        "# 2. CARGAR ARQUITECTURA\n",
        "model_checkpoint = \"xlm-roberta-base\"\n",
        "\n",
        "tokenizer_xlm = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model_xlm = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=1)\n",
        "\n",
        "# 3. PREPARACI√ìN DE DATOS DE ENTRENAMIENTO (INGL√âS)\n",
        "def preprocess_xlm(examples):\n",
        "    # El dataset ingl√©s usa 'score'\n",
        "    label_col = 'score' if 'score' in examples else 'similarity_score'\n",
        "    tokenized = tokenizer_xlm(\n",
        "        examples['sentence1'],\n",
        "        examples['sentence2'],\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "    tokenized['label'] = [float(s) for s in examples[label_col]]\n",
        "    return tokenized\n",
        "\n",
        "print(\"Cargando datos de ENTRENAMIENTO (Ingl√©s)\")\n",
        "dataset_en = load_dataset(\"mteb/stsbenchmark-sts\")\n",
        "\n",
        "print(\"Tokenizando datos de entrenamiento...\")\n",
        "encoded_train_en = dataset_en.map(preprocess_xlm, batched=True)\n",
        "\n",
        "# Formato Torch para el entrenamiento\n",
        "cols = ['input_ids', 'attention_mask', 'label']\n",
        "for split in ['train', 'validation']:\n",
        "    encoded_train_en[split].set_format(\"torch\", columns=cols)\n",
        "\n",
        "# 4. CONFIGURACI√ìN DEL ENTRENAMIENTO\n",
        "args_xlm = TrainingArguments(\n",
        "    output_dir=\"modelo_z3_xlm_roberta\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,              # XLM-R suele preferir LRs conservadores\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=4,   # Batch efectivo = 32\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=10,             # Con margen para el Early Stopping\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.1,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"pearson\",\n",
        "    greater_is_better=True,\n",
        "    save_total_limit=2,\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    if isinstance(predictions, tuple): predictions = predictions[0]\n",
        "    return {\"pearson\": pearsonr(predictions.flatten(), labels.flatten())[0]}\n",
        "\n",
        "trainer_xlm = Trainer(\n",
        "    model=model_xlm,\n",
        "    args=args_xlm,\n",
        "    train_dataset=encoded_train_en[\"train\"],\n",
        "    eval_dataset=encoded_train_en[\"validation\"],\n",
        "    tokenizer=tokenizer_xlm,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "\n",
        "# 5. EJECUTAR ENTRENAMIENTO\n",
        "print(\"Iniciando entrenamiento\")\n",
        "trainer_xlm.train()\n",
        "\n",
        "print(\"Entrenamiento finalizado\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdnzUPh9Bxo0"
      },
      "source": [
        "_3.3 EVALUACI√ìN FINAL Y CAMBIOS A REALIZAR PARA EL ENTRENAMIENTO_\n",
        "\n",
        "Utilizando los parametros iniciales del entrenamiento, los que me funcionaron bien en el entrenamiento anterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUwku8kzBx33"
      },
      "outputs": [],
      "source": [
        "\n",
        "# BLOQUE 3.3: EVALUACI√ìN FINAL Z3 (Test en Espa√±ol)\n",
        "\n",
        "# 1. CARGAR Y PREPARAR DATOS DE TEST (ESPA√ëOL)\n",
        "print(\"Preparando datos de TEST (Espa√±ol)\")\n",
        "if 'dataset_es' not in globals():\n",
        "    from datasets import load_dataset\n",
        "    try:\n",
        "        dataset_es = load_dataset(\"stsb_multi_mt\", name=\"es\", split=\"test\")\n",
        "    except:\n",
        "        dataset_es = load_dataset(\"mteb/stsbenchmark-sts\", \"es\", split=\"test\")\n",
        "\n",
        "# Tokenizador de memoria\n",
        "def preprocess_xlm_test(examples):\n",
        "    label_col = 'score' if 'score' in examples else 'similarity_score'\n",
        "    tokenized = tokenizer_xlm(\n",
        "        examples['sentence1'],\n",
        "        examples['sentence2'],\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "    tokenized['label'] = [float(s) for s in examples[label_col]]\n",
        "    return tokenized\n",
        "\n",
        "encoded_test_es = dataset_es.map(preprocess_xlm_test, batched=True)\n",
        "encoded_test_es.set_format(\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "# 2. EVALUACI√ìN\n",
        "print(\"\\n Evaluando modelo XLM-R Base en ESPA√ëOL\")\n",
        "# Uso el 'trainer_xlm' que ya tiene el mdelo cargado\n",
        "results_xlm = trainer_xlm.predict(encoded_test_es)\n",
        "pearson_xlm = results_xlm.metrics['test_pearson']\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(f\"RESULTADO FINAL Z3 (XLM-R Base): {pearson_xlm:.4f}\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCN-m0BUnq-x"
      },
      "source": [
        "**EXPERIMENTOS PARA ESTABLECER LOS MEJORES PAR√ÅMETROS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmYuzvlBnzxP"
      },
      "outputs": [],
      "source": [
        "\n",
        "# BLOQUE 0: CARGA DE DATOS COM√öN (Ejecutar UNA vez)\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "from transformers import AutoTokenizer, set_seed\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 1. Configuraci√≥n Global\n",
        "model_checkpoint = \"xlm-roberta-base\"\n",
        "tokenizer_xlm = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "# 2. Cargar Datos (si no est√°n ya)\n",
        "print(\"Cargando datos comunes\")\n",
        "dataset_en = load_dataset(\"mteb/stsbenchmark-sts\")\n",
        "try:\n",
        "    dataset_es = load_dataset(\"stsb_multi_mt\", name=\"es\", split=\"test\")\n",
        "except:\n",
        "    dataset_es = load_dataset(\"mteb/stsbenchmark-sts\", \"es\", split=\"test\")\n",
        "\n",
        "# 3. Tokenizar todo de una vez\n",
        "def preprocess_xlm(examples):\n",
        "    label_col = 'score' if 'score' in examples else 'similarity_score'\n",
        "    tokenized = tokenizer_xlm(\n",
        "        examples['sentence1'],\n",
        "        examples['sentence2'],\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "    tokenized['label'] = [float(s) for s in examples[label_col]]\n",
        "    return tokenized\n",
        "\n",
        "print(\" Tokenizando\")\n",
        "encoded_train = dataset_en.map(preprocess_xlm, batched=True)\n",
        "encoded_test_es = dataset_es.map(preprocess_xlm, batched=True)\n",
        "\n",
        "# Formato Torch\n",
        "cols = ['input_ids', 'attention_mask', 'label']\n",
        "for split in ['train', 'validation']:\n",
        "    encoded_train[split].set_format(\"torch\", columns=cols)\n",
        "encoded_test_es.set_format(\"torch\", columns=cols)\n",
        "\n",
        "print(\"Carga completa\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzCTjagxoY3k"
      },
      "outputs": [],
      "source": [
        "# EXP 1: BASELINE (Referencia)\n",
        "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Limpieza\n",
        "torch.cuda.empty_cache(); gc.collect(); set_seed(42)\n",
        "\n",
        "print(\"\\n INICIANDO EXP 1: BASELINE\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=1)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"z3_exp1_base\",\n",
        "    eval_strategy=\"epoch\", save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,              # LR Est√°ndar\n",
        "    per_device_train_batch_size=8, gradient_accumulation_steps=4,\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.01, warmup_ratio=0.1,\n",
        "    load_best_model_at_end=True, metric_for_best_model=\"pearson\", greater_is_better=True,\n",
        "    save_total_limit=1, fp16=True, report_to=\"none\"\n",
        ")\n",
        "\n",
        "def compute_metrics(p): return {\"pearson\": pearsonr(p.predictions.flatten(), p.label_ids.flatten())[0]}\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model, args=args,\n",
        "    train_dataset=encoded_train[\"train\"], eval_dataset=encoded_train[\"validation\"],\n",
        "    tokenizer=tokenizer_xlm, compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "res = trainer.predict(encoded_test_es)\n",
        "print(f\" RESULTADO EXP 1 (Baseline): {res.metrics['test_pearson']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zK1ReI0_omIe"
      },
      "outputs": [],
      "source": [
        "# --- EXP 2: LR ALTO ---\n",
        "torch.cuda.empty_cache(); gc.collect(); set_seed(42)\n",
        "\n",
        "print(\"\\n INICIANDO EXP 2: HIGH LEARNING RATE\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=1)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"z3_exp2_high_lr\",\n",
        "    eval_strategy=\"epoch\", save_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,              # CAMBIO: 2.5x m√°s r√°pido\n",
        "    per_device_train_batch_size=8, gradient_accumulation_steps=4,\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.01, warmup_ratio=0.1,\n",
        "    load_best_model_at_end=True, metric_for_best_model=\"pearson\", greater_is_better=True,\n",
        "    save_total_limit=1, fp16=True, report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model, args=args,\n",
        "    train_dataset=encoded_train[\"train\"], eval_dataset=encoded_train[\"validation\"],\n",
        "    tokenizer=tokenizer_xlm, compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "res = trainer.predict(encoded_test_es)\n",
        "print(f\" RESULTADO EXP 2 (High LR 5e-5): {res.metrics['test_pearson']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sop_Fesyoo3v"
      },
      "outputs": [],
      "source": [
        "# --- EXP 3: LR BAJO  ---\n",
        "torch.cuda.empty_cache(); gc.collect(); set_seed(42)\n",
        "\n",
        "print(\"\\n INICIANDO EXP 3: LOW LEARNING RATE\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=1)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"z3_exp3_low_lr\",\n",
        "    eval_strategy=\"epoch\", save_strategy=\"epoch\",\n",
        "    learning_rate=1e-5,              # CAMBIO Mitad de velocidad\n",
        "    per_device_train_batch_size=8, gradient_accumulation_steps=4,\n",
        "    num_train_epochs=12,             # CAMBI Un poco m√°s de tiempo para compensar\n",
        "    weight_decay=0.01, warmup_ratio=0.1,\n",
        "    load_best_model_at_end=True, metric_for_best_model=\"pearson\", greater_is_better=True,\n",
        "    save_total_limit=1, fp16=True, report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model, args=args,\n",
        "    train_dataset=encoded_train[\"train\"], eval_dataset=encoded_train[\"validation\"],\n",
        "    tokenizer=tokenizer_xlm, compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "res = trainer.predict(encoded_test_es)\n",
        "print(f\" RESULTADO EXP 3 (Low LR 1e-5): {res.metrics['test_pearson']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gI5vqYzPoprx"
      },
      "outputs": [],
      "source": [
        "# --- EXPERIMENTO 4: HIGH LEARNING RATE (5e-5) ---\n",
        "# Hip√≥tesis: Un learning rate m√°s alto podr√≠a permitir al modelo adaptarse\n",
        "# m√°s r√°pido a la tarea, aunque corre riesgo de inestabilidad.\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback, set_seed\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# 1. Limpieza\n",
        "if 'model' in globals(): del model\n",
        "if 'trainer' in globals(): del trainer\n",
        "torch.cuda.empty_cache(); gc.collect(); set_seed(42)\n",
        "\n",
        "# 2. Modelo Nuevo\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=1)\n",
        "\n",
        "# 3. Configuraci√≥n\n",
        "args_exp4 = TrainingArguments(\n",
        "    output_dir=\"z3_exp4_high_lr\",\n",
        "    eval_strategy=\"epoch\", save_strategy=\"epoch\",\n",
        "\n",
        "    learning_rate=5e-5,              # CAMBIO M√°s alto (antes era 2e-5)\n",
        "\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=4,   # Batch efectivo = 32\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.01, warmup_ratio=0.1,\n",
        "    load_best_model_at_end=True, metric_for_best_model=\"pearson\", greater_is_better=True,\n",
        "    save_total_limit=1, fp16=True, report_to=\"none\"\n",
        ")\n",
        "#DEFINIR METRICA\n",
        "def compute_metrics(p): return {\"pearson\": pearsonr(p.predictions.flatten(), p.label_ids.flatten())[0]}\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model, args=args_exp4,\n",
        "    train_dataset=encoded_train[\"train\"], eval_dataset=encoded_train[\"validation\"],\n",
        "    tokenizer=tokenizer_xlm, compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "res = trainer.predict(encoded_test_es)\n",
        "print(f\" RESULTADO EXP 4 (High LR): {res.metrics['test_pearson']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EboSuC3eospX"
      },
      "outputs": [],
      "source": [
        "# --- EXPERIMENTO 5: SMALL BATCH SIZE (Sin Acumulaci√≥n) ---\n",
        "# Hip√≥tesis: Al quitar la acumulaci√≥n de gradientes, el modelo actualiza sus pesos\n",
        "# con cada batch de 8 ejemplos (m√°s ruido). Quiero ver si esto ayuda o perjudica.\n",
        "\n",
        "# 1. Limpieza\n",
        "torch.cuda.empty_cache(); gc.collect(); set_seed(42)\n",
        "\n",
        "print(\"\\n INICIANDO EXP 5: BATCH SIZE PEQUE√ëO (8)\")\n",
        "\n",
        "# 2. Modelo Nuevo\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=1)\n",
        "\n",
        "# 3. Configuraci√≥n\n",
        "args_exp5 = TrainingArguments(\n",
        "    output_dir=\"z3_exp5_small_batch\",\n",
        "    eval_strategy=\"epoch\", save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,              # LR normal\n",
        "\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=1,   # CAMBIO: Antes 4 (Batch 32), ahora 1 (Batch 8)\n",
        "                                     # Esto hace el entrenamiento m√°s \"inestable\" pero a veces explora mejor.\n",
        "\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.01, warmup_ratio=0.1,\n",
        "    load_best_model_at_end=True, metric_for_best_model=\"pearson\", greater_is_better=True,\n",
        "    save_total_limit=1, fp16=True, report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model, args=args_exp5,\n",
        "    train_dataset=encoded_train[\"train\"], eval_dataset=encoded_train[\"validation\"],\n",
        "    tokenizer=tokenizer_xlm, compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "res = trainer.predict(encoded_test_es)\n",
        "print(f\" RESULTADO EXP 5 (Small Batch): {res.metrics['test_pearson']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UB2Of0ACowZQ"
      },
      "outputs": [],
      "source": [
        "# --- EXP 6: HIGH STABILITY ---\n",
        "torch.cuda.empty_cache(); gc.collect(); set_seed(42)\n",
        "\n",
        "print(\"\\n INICIANDO EXP 6\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=1)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"z3_exp6_stable\",\n",
        "    eval_strategy=\"epoch\", save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=8,   # CAMBIO: Batch Efectivo = 64\n",
        "    num_train_epochs=20,             # CAMBIO: Hasta 20 epoches\n",
        "    weight_decay=0.01, warmup_ratio=0.1,\n",
        "    load_best_model_at_end=True, metric_for_best_model=\"pearson\", greater_is_better=True,\n",
        "    save_total_limit=1, fp16=True, report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model, args=args,\n",
        "    train_dataset=encoded_train[\"train\"], eval_dataset=encoded_train[\"validation\"],\n",
        "    tokenizer=tokenizer_xlm, compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)] # CAMBIO EN EL EAERLY STOPPING\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "res = trainer.predict(encoded_test_es)\n",
        "print(f\"RESULTADO EXP 6 (High Stability): {res.metrics['test_pearson']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ULTIMOS EXPERIEMNTOS: VER LA TRANSFERENCIA ENTRE DISTINTOS IDIOMAS QUE PUEDAN TENER DISNTINTA FORMA DEL ESPA√ëOL Y VER COMO REACCIONAN - TESTEAR TRASNGFERENCIA**\n",
        "\n",
        "_BLOQUE 1: Preparaci√≥n para los √∫ltimos experimentos_"
      ],
      "metadata": {
        "id": "XfHJuSrCBDhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BLOQUE 0: PREPARACI√ìN ---\n",
        "from datasets import concatenate_datasets, load_dataset\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback, AutoTokenizer\n",
        "from scipy.stats import pearsonr\n",
        "import numpy as np\n",
        "\n",
        "# 1. Cargar Tokenizer\n",
        "tokenizer_xlm = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "\n",
        "# 2. Funciones Auxiliares\n",
        "def get_multilingual_data(languages, split):\n",
        "    \"\"\"Descarga y combina idiomas desde stsb_multi_mt\"\"\"\n",
        "    datasets_list = []\n",
        "    print(f\"Cargando datos para: {languages} (split: {split})\")\n",
        "    for lang in languages:\n",
        "        try:\n",
        "            ds = load_dataset(\"mteb/stsb_multi_mt\", name=lang, split=split)\n",
        "            if 'similarity_score' in ds.features:\n",
        "                 ds = ds.rename_column(\"similarity_score\", \"label\")\n",
        "            datasets_list.append(ds)\n",
        "        except Exception as e:\n",
        "            print(f\"Error cargando idioma {lang}: {e}\")\n",
        "    return concatenate_datasets(datasets_list) if datasets_list else None\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer_xlm(examples['sentence1'], examples['sentence2'], truncation=True, max_length=128)\n",
        "\n",
        "def compute_metrics(p):\n",
        "    return {\"pearson\": pearsonr(p.predictions.flatten(), p.label_ids.flatten())[0]}\n",
        "\n",
        "# 3. Configuraci√≥n de entrenamiento manteniendo mejores hiperpar√°metros\n",
        "common_args = TrainingArguments(\n",
        "    output_dir=\"checkpoints_temp\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=6,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"pearson\",\n",
        "    greater_is_better=True,\n",
        "    save_total_limit=1,\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "print(\"Configuraci√≥n lista.\")"
      ],
      "metadata": {
        "id": "JOESVKz4CKoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_PRIMER EXPERIMENTO: ENTRENAMIENTO EN INGL√âS Y CASTELLANO Y POSTERIOR TEST EN RUSO_\n",
        "\n",
        "(generalizaci√≥n en distinto alfab√©to: cir√≠lico)"
      ],
      "metadata": {
        "id": "SJKQEUJqCYnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EXPERIMENTO 1: TRAIN [ES + EN] -> TEST [RU]\n",
        "print(\"\\n INICIANDO EXP 1: Pol√≠glota (ES+EN) probando en RUSO\")\n",
        "\n",
        "# 1. Datos\n",
        "train_ds = get_multilingual_data(['es', 'en'], 'train').map(preprocess_function, batched=True)\n",
        "val_ds   = get_multilingual_data(['es', 'en'], 'dev').map(preprocess_function, batched=True)\n",
        "test_ds  = get_multilingual_data(['ru'], 'test').map(preprocess_function, batched=True)\n",
        "\n",
        "# 2. Inicializar modelo pre entrebado\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=1)\n",
        "\n",
        "# 3. Entrenar\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=common_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    tokenizer=tokenizer_xlm,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "# 4. Evaluar\n",
        "res = trainer.predict(test_ds)\n",
        "print(f\"\\nRESULTADO FINAL EXP 1 (Test RUSO): {res.metrics['test_pearson']:.4f}\")"
      ],
      "metadata": {
        "id": "Ylnq44XRCmCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_EXPERIMENTO 2: TRAIN EN ESPA√ëOL Y TRANSFERENCIA A CHINO_\n",
        "\n",
        "De alfabeto latino a logogramas chinos."
      ],
      "metadata": {
        "id": "NPGczyiiCvcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- EXPERIMENTO 2: TRAIN [ES] -> TEST [ZH] (Chino) ---\n",
        "print(\"\\n INICIANDO EXP 2: Espa√±ol puro probando en CHINO\")\n",
        "\n",
        "# 1. Datos\n",
        "train_ds = get_multilingual_data(['es'], 'train').map(preprocess_function, batched=True)\n",
        "val_ds   = get_multilingual_data(['es'], 'dev').map(preprocess_function, batched=True)\n",
        "test_ds  = get_multilingual_data(['zh'], 'test').map(preprocess_function, batched=True)\n",
        "\n",
        "# 2. Modelo Limpio para no usar lo aprendido anteriormente\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=1)\n",
        "\n",
        "# 3. Entrenar\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=common_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    tokenizer=tokenizer_xlm,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "# 4. Evaluar\n",
        "res = trainer.predict(test_ds)\n",
        "print(f\"\\n RESULTADO FINAL EXP 2 (Test CHINO): {res.metrics['test_pearson']:.4f}\")"
      ],
      "metadata": {
        "id": "N6OdLYi8C2wU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_EXPERIMENTO 3: VUELTA CON RESPECTO A LOS EXPERIEMENTOS INICIALES -testear como se mantiene la tranferencia invirtiendo espa√±ol e ingl√©s_\n",
        "\n",
        "Transferencia inversa: vamos a entrenar en espa√±ol y probar en ingl√©s"
      ],
      "metadata": {
        "id": "hOUJsVSRC7KU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EXPERIMENTO 3: TRAIN [ES] -> TEST [EN]\n",
        "print(\"\\n INICIANDO EXP 3: Espa√±ol puro probando en INGL√âS\")\n",
        "\n",
        "# 1. Datos\n",
        "train_ds = get_multilingual_data(['es'], 'train').map(preprocess_function, batched=True)\n",
        "val_ds   = get_multilingual_data(['es'], 'dev').map(preprocess_function, batched=True)\n",
        "test_ds  = get_multilingual_data(['en'], 'test').map(preprocess_function, batched=True)\n",
        "\n",
        "# 2. Modelo Limpio\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=1)\n",
        "\n",
        "# 3. Entrenar\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=common_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    tokenizer=tokenizer_xlm,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "# 4. Evaluar\n",
        "res = trainer.predict(test_ds)\n",
        "print(f\"\\n RESULTADO FINAL EXP 3 (Test INGL√âS): {res.metrics['test_pearson']:.4f}\")"
      ],
      "metadata": {
        "id": "0js8exvLDJOL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}